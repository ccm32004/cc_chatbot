# -*- coding: utf-8 -*-
"""cece_chat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qdDQPbIoMEktCx8zUrVOOl9vZYTdIbqI
"""

!pip install -q transformers datasets peft trl accelerate bitsandbytes huggingface_hub

!pip uninstall -y datasets fsspec gcsfs

!pip install fsspec==2025.3.0
!pip install gcsfs==2025.3.2
!pip install datasets==3.6.0

import fsspec, gcsfs, datasets
print("fsspec:", fsspec.__version__)
print("gcsfs:", gcsfs.__version__)
print("datasets:", datasets.__version__)

from huggingface_hub import login
login(token="INSERT TOKEN HERE")

from google.colab import files

# Prompt file upload
uploaded = files.upload()

# Optional: check the file
for fn in uploaded.keys():
    print(f"Uploaded file: {fn}")

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer

def format(example):
    return {
        "text": f"Q: {example['prompt']}\nA: {example['response']}"
    }

# Load your JSONL dataset
dataset = load_dataset("json", data_files="cece_dataset.jsonl", split="train")
dataset = dataset.map(format)
dataset = dataset.rename_column("response", "completion")


# Load DialoGPT-small
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
# model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map="auto")
# tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # Important!

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,  # LoRA + memory efficient
    device_map="auto"
)

# Set up LoRA config
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none"
)
model = get_peft_model(model, peft_config)

# Train the model
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=TrainingArguments(
        output_dir="./cece-chatbot",
        per_device_train_batch_size=2,
        num_train_epochs=3,
        save_total_limit=1,
        logging_dir="./logs",
        logging_steps=10,
        report_to="none",
    ),
)
trainer.train()

model.push_to_hub("ccm32004/cece-chatbot")
tokenizer.push_to_hub("ccm32004/cece-chatbot")

!pip install -q gradio

import gradio as gr
from transformers import pipeline

chat = pipeline("text-generation", model="ccm32004/cece-chatbot")

system_prompt = """
You are answering questions for Cece, a software developer passionate about full-stack projects, DevOps, and hiking. Only answer questions based on known projects, personality, and preferences.
"""

def respond(message, history):
    prompt = f"{system_prompt}\n\nQ: {message}\nA:"
    result = chat(prompt, max_new_tokens=100, pad_token_id=tokenizer.pad_token_id)[0]['generated_text']
    full_output = result[len(prompt):].strip()

    # Truncate the output if it starts hallucinating new Q&As
    for stop_token in ["\nQ:", "\nA:", "\n\n"]:
        if stop_token in full_output:
            full_output = full_output.split(stop_token)[0].strip()
            break

    return full_output

gr.ChatInterface(respond).launch()

!nvidia-smi